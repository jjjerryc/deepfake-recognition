#!/usr/bin/env python3
"""
Deepfake Detection Baseline
============================

æ¥µç°¡ç‰ˆè¨“ç·´ + æ¨è«–è…³æœ¬ï¼Œä½¿ç”¨ CLIP ViT-B/32 ä½œç‚º backboneã€‚

ä½¿ç”¨æ–¹å¼:
    # è¨“ç·´
    python baseline.py --mode train
    
    # æ¨è«–
    python baseline.py --mode inference
    
    # è¨“ç·´ + æ¨è«–
    python baseline.py --mode both

ç‰¹é»:
    - å–®ä¸€æª”æ¡ˆï¼Œç´„ 300 è¡Œ
    - CLIP ViT-B/32 å‡çµ backboneï¼Œåªè¨“ç·´åˆ†é¡é ­
    - è¼¸å‡º Kaggle æ ¼å¼ (filename, real/fake)
    - æ”¯æ´ threshold èª¿æ•´å’Œ 50/50 å¼·åˆ¶åˆ†å¸ƒ
"""

import argparse
import os
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
from PIL import Image
from tqdm import tqdm

# ============================================================================
# é…ç½®
# ============================================================================

CONFIG = {
    # è³‡æ–™
    'data_path': './dataset',
    'image_size': 224,
    'val_split': 0.15,
    
    # è¨“ç·´
    'batch_size': 128,
    'epochs': 30,
    'lr': 1e-3,
    'weight_decay': 0.01,
    'patience': 8,
    
    # è¼¸å‡º
    'output_dir': './outputs/baseline',
    'seed': 42,
}

# CLIP é è™•ç†åƒæ•¸
CLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]
CLIP_STD = [0.26862954, 0.26130258, 0.27577711]

# æ¨™ç±¤æ˜ å°„ï¼ˆæŒ‰å­—æ¯é †åºï¼šfake=0, real=1ï¼‰
LABEL_MAP = {0: 'fake', 1: 'real'}


# ============================================================================
# æ¨¡å‹
# ============================================================================

class CLIPClassifier(nn.Module):
    """
    CLIP ViT-B/32 + ç·šæ€§åˆ†é¡é ­
    å‡çµ CLIP backboneï¼Œåªè¨“ç·´åˆ†é¡é ­
    """
    
    def __init__(self, num_classes: int = 2, dropout: float = 0.3):
        super().__init__()
        
        # è¼‰å…¥ CLIP
        import open_clip
        self.clip_model, _, _ = open_clip.create_model_and_transforms(
            'ViT-B-32', pretrained='openai'
        )
        
        # å‡çµ CLIP
        for param in self.clip_model.parameters():
            param.requires_grad = False
        self.clip_model.eval()
        
        # ç²å–ç‰¹å¾µç¶­åº¦
        self.embed_dim = self.clip_model.visual.output_dim  # 512
        
        # åˆ†é¡é ­
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.embed_dim),
            nn.Dropout(dropout),
            nn.Linear(self.embed_dim, num_classes)
        )
        
        print(f"âœ… CLIP ViT-B/32 loaded (frozen)")
        print(f"   Embed dim: {self.embed_dim}")
        print(f"   Trainable params: {sum(p.numel() for p in self.classifier.parameters()):,}")
    
    def forward(self, x):
        with torch.no_grad():
            features = self.clip_model.encode_image(x).float()
        return self.classifier(features)


# ============================================================================
# è³‡æ–™
# ============================================================================

def get_transforms():
    """ç²å–è¨“ç·´å’Œæ¸¬è©¦ç”¨çš„ transforms"""
    train_transform = transforms.Compose([
        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD),
    ])
    
    test_transform = transforms.Compose([
        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),
        transforms.ToTensor(),
        transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD),
    ])
    
    return train_transform, test_transform


def get_dataloaders():
    """è¼‰å…¥è¨“ç·´å’Œé©—è­‰è³‡æ–™"""
    train_transform, _ = get_transforms()
    
    # ä½¿ç”¨ ImageFolder è‡ªå‹•è®€å– train/fake å’Œ train/real
    train_dir = os.path.join(CONFIG['data_path'], 'train')
    full_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
    
    # ç¢ºèªé¡åˆ¥é †åº
    print(f"ğŸ“ Classes: {full_dataset.classes}")  # æ‡‰è©²æ˜¯ ['fake', 'real']
    print(f"ğŸ“Š Total samples: {len(full_dataset)}")
    
    # åˆ†å‰²è¨“ç·´/é©—è­‰
    val_size = int(len(full_dataset) * CONFIG['val_split'])
    train_size = len(full_dataset) - val_size
    
    torch.manual_seed(CONFIG['seed'])
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    print(f"   Train: {len(train_dataset)}, Val: {len(val_dataset)}")
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=True,
        num_workers=8,
        pin_memory=True,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=False,
        num_workers=8,
        pin_memory=True,
    )
    
    return train_loader, val_loader


class TestDataset(torch.utils.data.Dataset):
    """æ¸¬è©¦è³‡æ–™é›†"""
    
    def __init__(self, test_dir, transform):
        self.test_dir = Path(test_dir)
        self.transform = transform
        
        valid_ext = {'.jpg', '.jpeg', '.png'}
        self.images = sorted(
            [f for f in self.test_dir.iterdir() if f.suffix.lower() in valid_ext],
            key=lambda x: int(x.stem) if x.stem.isdigit() else x.stem
        )
        print(f"ğŸ“ Test images: {len(self.images)}")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        img_path = self.images[idx]
        image = Image.open(img_path).convert('RGB')
        image = self.transform(image)
        return image, img_path.stem


# ============================================================================
# è¨“ç·´
# ============================================================================

def train():
    """è¨“ç·´æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("ğŸš€ Training Baseline Model")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # è³‡æ–™
    train_loader, val_loader = get_dataloaders()
    
    # æ¨¡å‹
    model = CLIPClassifier(num_classes=2, dropout=0.3).to(device)
    
    # å„ªåŒ–å™¨ï¼ˆåªå„ªåŒ–åˆ†é¡é ­ï¼‰
    optimizer = torch.optim.AdamW(
        model.classifier.parameters(),
        lr=CONFIG['lr'],
        weight_decay=CONFIG['weight_decay']
    )
    
    # æå¤±å‡½æ•¸
    criterion = nn.CrossEntropyLoss()
    
    # è¨“ç·´
    best_val_acc = 0.0
    patience_counter = 0
    
    os.makedirs(CONFIG['output_dir'], exist_ok=True)
    
    for epoch in range(CONFIG['epochs']):
        # Train
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']}")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100.*train_correct/train_total:.2f}%'
            })
        
        train_acc = 100. * train_correct / train_total
        
        # Validation
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        val_acc = 100. * val_correct / val_total
        
        print(f"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")
        
        # Early stopping
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            
            # å„²å­˜æœ€ä½³æ¨¡å‹
            save_path = os.path.join(CONFIG['output_dir'], 'best_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
            }, save_path)
            print(f"ğŸ’¾ Saved best model (val_acc: {val_acc:.2f}%)")
        else:
            patience_counter += 1
            if patience_counter >= CONFIG['patience']:
                print(f"â¹ï¸ Early stopping at epoch {epoch+1}")
                break
    
    print(f"\nâœ… Training complete! Best Val Acc: {best_val_acc:.2f}%")
    return best_val_acc


# ============================================================================
# æ¨è«–
# ============================================================================

@torch.no_grad()
def inference(use_median_threshold: bool = True):
    """æ¨è«–ä¸¦ç”Ÿæˆæäº¤æª”æ¡ˆ"""
    print("\n" + "=" * 60)
    print("ğŸ”® Running Inference")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # è¼‰å…¥æ¨¡å‹
    model = CLIPClassifier(num_classes=2).to(device)
    checkpoint_path = os.path.join(CONFIG['output_dir'], 'best_model.pth')
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ Model not found: {checkpoint_path}")
        print("   Please run training first: python baseline.py --mode train")
        return
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ… Loaded model (val_acc: {checkpoint['val_acc']:.2f}%)")
    
    # è³‡æ–™
    _, test_transform = get_transforms()
    test_dir = os.path.join(CONFIG['data_path'], 'test')
    test_dataset = TestDataset(test_dir, test_transform)
    test_loader = DataLoader(
        test_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=False,
        num_workers=8,
        pin_memory=True,
    )
    
    # æ¨è«–
    all_filenames = []
    all_probs = []
    
    for images, filenames in tqdm(test_loader, desc="Inference"):
        images = images.to(device)
        outputs = model(images)
        probs = F.softmax(outputs, dim=1)
        
        # å– fake çš„æ©Ÿç‡ (index 0)
        fake_probs = probs[:, 0].cpu().numpy()
        
        all_filenames.extend(filenames)
        all_probs.extend(fake_probs)
    
    all_probs = np.array(all_probs)
    
    # æ±ºå®š threshold
    if use_median_threshold:
        threshold = np.median(all_probs)
        print(f"ğŸ“Š Using median threshold: {threshold:.4f} (forces 50/50 split)")
    else:
        threshold = 0.5
        print(f"ğŸ“Š Using threshold: {threshold}")
    
    # ç”Ÿæˆæ¨™ç±¤
    labels = ['fake' if p > threshold else 'real' for p in all_probs]
    
    # çµ±è¨ˆ
    fake_count = labels.count('fake')
    real_count = labels.count('real')
    total = len(labels)
    
    print(f"\nğŸ“ˆ Distribution:")
    print(f"   Fake: {fake_count} ({fake_count/total*100:.1f}%)")
    print(f"   Real: {real_count} ({real_count/total*100:.1f}%)")
    
    # å„²å­˜
    os.makedirs(CONFIG['output_dir'], exist_ok=True)
    
    # Kaggle æ ¼å¼
    submission_path = os.path.join(CONFIG['output_dir'], 'submission.csv')
    with open(submission_path, 'w') as f:
        f.write("filename,label\n")
        for fname, label in zip(all_filenames, labels):
            f.write(f"{fname},{label}\n")
    print(f"\nâœ… Saved: {submission_path}")
    
    # æ©Ÿç‡ç‰ˆæœ¬ï¼ˆæ–¹ä¾¿èª¿æ•´ thresholdï¼‰
    probs_path = os.path.join(CONFIG['output_dir'], 'predictions_probs.csv')
    with open(probs_path, 'w') as f:
        f.write("filename,fake_prob\n")
        for fname, prob in zip(all_filenames, all_probs):
            f.write(f"{fname},{prob:.6f}\n")
    print(f"âœ… Saved: {probs_path}")
    
    print("\nâœ… Inference complete!")


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='Deepfake Detection Baseline')
    parser.add_argument('--mode', type=str, default='both',
                        choices=['train', 'inference', 'both'],
                        help='train, inference, or both')
    parser.add_argument('--threshold', type=float, default=None,
                        help='Custom threshold (default: use median for 50/50)')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ”¬ Deepfake Detection Baseline")
    print("   Model: CLIP ViT-B/32 (frozen) + Linear Classifier")
    print("=" * 60)
    
    if args.mode in ['train', 'both']:
        train()
    
    if args.mode in ['inference', 'both']:
        use_median = args.threshold is None
        inference(use_median_threshold=use_median)


if __name__ == '__main__':
    main()
