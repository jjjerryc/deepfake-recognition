"""
DINOv2 Deepfake Detection Model

ä½¿ç”¨ DINOv2 ViT-Large/14 ä½œç‚º backboneï¼Œ
æ”¯æ´å·®ç•°åŒ–è§£å‡ç­–ç•¥å’Œå¤šç¨®æ± åŒ–æ–¹å¼ã€‚

DINOv2 çš„å„ªå‹¢ï¼š
1. è‡ªç›£ç£å­¸ç¿’ï¼Œå°åº•å±¤è¦–è¦ºçµæ§‹æ•æ„Ÿ
2. èƒ½æ•æ‰ç´°å¾®çš„ç´‹ç†ä¸ä¸€è‡´å’Œé‚Šç·£å½å½±
3. æ¯” CLIP æ›´é©åˆåƒç´ ç´šåˆ¥çš„å½é€ æª¢æ¸¬
"""

from typing import Dict, Any, Optional

import torch
import torch.nn as nn

from .base import BaseModel


class DINOv2Classifier(BaseModel):
    """
    DINOv2 Visual Encoder + Classification Head
    
    æ”¯æ´ï¼š
    - å·®ç•°åŒ–è§£å‡ç­–ç•¥ (è§£å‡æœ€å¾Œ N å±¤)
    - å¤šç¨®æ± åŒ–æ–¹å¼ (cls token / patch mean)
    - åˆ†å±¤å­¸ç¿’ç‡
    """
    
    def __init__(
        self,
        backbone: str = "dinov2_vitl14",
        num_classes: int = 2,
        unfreeze_layers: int = 2,
        unfreeze_norm: bool = True,
        pooling: str = "cls",  # "cls" or "avg"
        hidden_dim: int = 512,
        dropout: float = 0.4,
        use_batchnorm: bool = True,
    ):
        super().__init__(num_classes=num_classes)
        
        self.backbone_name = backbone
        self.unfreeze_layers = unfreeze_layers
        self.pooling = pooling
        
        # è¼‰å…¥ DINOv2
        print(f"Loading {backbone}...")
        self.backbone = torch.hub.load(
            'facebookresearch/dinov2', 
            backbone,
            pretrained=True
        )
        
        # ç²å– embedding ç¶­åº¦
        if backbone == "dinov2_vitl14":
            self.embed_dim = 1024
        elif backbone == "dinov2_vitb14":
            self.embed_dim = 768
        elif backbone == "dinov2_vits14":
            self.embed_dim = 384
        elif backbone == "dinov2_vitg14":
            self.embed_dim = 1536
        else:
            # å‹•æ…‹æ¨æ–·
            with torch.no_grad():
                dummy = torch.zeros(1, 3, 224, 224)
                out = self.backbone(dummy)
                self.embed_dim = out.shape[-1]
        
        # è¨­å®šå‡çµç­–ç•¥
        self._setup_freeze(unfreeze_norm)
        
        # åˆ†é¡é ­
        if use_batchnorm:
            self.classifier = nn.Sequential(
                nn.Linear(self.embed_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, num_classes if num_classes > 2 else 1),
            )
        else:
            self.classifier = nn.Sequential(
                nn.Linear(self.embed_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, num_classes if num_classes > 2 else 1),
            )
        
        self._print_info()
    
    def _setup_freeze(self, unfreeze_norm: bool = True):
        """å‡çµé™¤äº†æœ€å¾Œ N å±¤ä»¥å¤–çš„æ‰€æœ‰å±¤"""
        # å…ˆå‡çµæ‰€æœ‰
        for param in self.backbone.parameters():
            param.requires_grad = False
        
        # ç²å– transformer blocks
        if hasattr(self.backbone, 'blocks'):
            blocks = self.backbone.blocks
            total_blocks = len(blocks)
            
            if self.unfreeze_layers > 0:
                # è§£å‡æœ€å¾Œ N å±¤ Blocks
                unfreeze_start = total_blocks - self.unfreeze_layers
                for i in range(unfreeze_start, total_blocks):
                    for param in blocks[i].parameters():
                        param.requires_grad = True
                
                print(f"ğŸ”“ Unfreezing last {self.unfreeze_layers}/{total_blocks} transformer blocks")
        
        # è§£å‡ Norm å±¤
        if unfreeze_norm and hasattr(self.backbone, 'norm'):
            for param in self.backbone.norm.parameters():
                param.requires_grad = True
    
    def _print_info(self):
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        frozen_params = total_params - trainable_params
        
        print(f"\nğŸ“Š DINOv2 Model Statistics:")
        print(f"   Backbone: {self.backbone_name}")
        print(f"   Pooling: {self.pooling}")
        print(f"   Total params: {total_params:,}")
        print(f"   Trainable: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
        print(f"   Frozen: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.pooling == "avg":
            # ä½¿ç”¨ patch tokens çš„å¹³å‡å€¼
            features_dict = self.backbone.forward_features(x)
            features = features_dict['x_norm_patchtokens'].mean(dim=1)
        else:
            # ä½¿ç”¨ CLS token (é è¨­)
            features = self.backbone(x)
        
        return self.classifier(features)
    
    def get_classifier(self) -> nn.Module:
        return self.classifier
    
    def get_backbone(self) -> nn.Module:
        return self.backbone
    
    def get_preprocessing_config(self) -> Dict[str, Any]:
        return {
            'mean': [0.485, 0.456, 0.406],
            'std': [0.229, 0.224, 0.225],
            'input_size': 224,
            'interpolation': 'bicubic',
        }
    
    def get_param_groups(self, base_lr: float, backbone_lr_multiplier: float = 0.1) -> list:
        """åˆ†å±¤å­¸ç¿’ç‡"""
        param_groups = []
        
        # Backbone (è§£å‡çš„å±¤)
        backbone_params = [p for p in self.backbone.parameters() if p.requires_grad]
        if backbone_params:
            param_groups.append({
                'params': backbone_params,
                'lr': base_lr * backbone_lr_multiplier,
                'name': 'backbone'
            })
        
        # Classifier
        param_groups.append({
            'params': list(self.classifier.parameters()),
            'lr': base_lr,
            'name': 'classifier'
        })
        
        return param_groups


# DINOv2 æ¨¡å‹é…ç½®
DINO_CONFIGS = {
    'dino_vits14': {
        'backbone': 'dinov2_vits14',
        'embed_dim': 384,
    },
    'dino_vitb14': {
        'backbone': 'dinov2_vitb14',
        'embed_dim': 768,
    },
    'dino_vitl14': {
        'backbone': 'dinov2_vitl14',
        'embed_dim': 1024,
    },
    'dino_vitg14': {
        'backbone': 'dinov2_vitg14',
        'embed_dim': 1536,
    },
}
