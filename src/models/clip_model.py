"""
CLIP-based Deepfake Detection Model

ä½¿ç”¨ OpenCLIP çš„ visual encoder ä½œç‚º backbone
å‡çµ encoder åªè¨“ç·´åˆ†é¡é ­ï¼Œé”åˆ°æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

CLIP çš„å„ªå‹¢ï¼š
1. åœ¨å¤§é‡åœ–åƒ-æ–‡å­—é…å°æ•¸æ“šä¸Šé è¨“ç·´ï¼Œå­¸åˆ°æ›´é€šç”¨çš„è¦–è¦ºç‰¹å¾µ
2. å° "ä»€éº¼æ˜¯è‡ªç„¶åœ–åƒ" æœ‰æ·±åˆ»ç†è§£
3. æ›´å®¹æ˜“æ³›åŒ–åˆ°æœªè¦‹éçš„ç”Ÿæˆæ¨¡å‹
"""

from typing import Dict, Any, List, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    import open_clip
except ImportError:
    raise ImportError("Please install open_clip_torch: pip install open_clip_torch")

from .base import BaseModel


# æ”¯æ´çš„ CLIP æ¨¡å‹é…ç½®
CLIP_CONFIGS = {
    'clip_vit_b32': {
        'model_name': 'ViT-B-32',
        'pretrained': 'openai',
        'embed_dim': 512,
    },
    'clip_vit_b16': {
        'model_name': 'ViT-B-16',
        'pretrained': 'openai',
        'embed_dim': 512,
    },
    'clip_vit_l14': {
        'model_name': 'ViT-L-14',
        'pretrained': 'openai',
        'embed_dim': 768,
    },
    'clip_vit_b16_laion': {
        'model_name': 'ViT-B-16',
        'pretrained': 'laion2b_s34b_b88k',
        'embed_dim': 512,
    },
    'clip_convnext_base': {
        'model_name': 'convnext_base_w',
        'pretrained': 'laion2b_s13b_b82k',
        'embed_dim': 640,
    },
}


class CLIPClassifier(BaseModel):
    """
    CLIP Visual Encoder + Classification Head
    
    å‡çµ CLIP encoderï¼Œåªè¨“ç·´åˆ†é¡é ­
    """
    
    def __init__(
        self,
        clip_model: str = 'ViT-B-32',
        pretrained: str = 'openai',
        num_classes: int = 2,
        dropout: float = 0.5,
        freeze_encoder: bool = True,
        hidden_dim: int = 512,
    ):
        super().__init__(num_classes=num_classes)
        
        self.clip_model_name = clip_model
        self.pretrained = pretrained
        self.freeze_encoder = freeze_encoder
        
        # è¼‰å…¥ CLIP æ¨¡å‹
        print(f"Loading CLIP: {clip_model} ({pretrained})")
        self.clip, _, self.preprocess = open_clip.create_model_and_transforms(
            clip_model, pretrained=pretrained
        )
        
        # åªä¿ç•™ visual encoder
        self.encoder = self.clip.visual
        
        # ç²å–ç‰¹å¾µç¶­åº¦
        # å˜—è©¦ä¸åŒçš„æ–¹å¼ç²å– embed_dim
        if hasattr(self.encoder, 'output_dim'):
            self.embed_dim = self.encoder.output_dim
        elif hasattr(self.encoder, 'embed_dim'):
            self.embed_dim = self.encoder.embed_dim
        elif hasattr(self.clip, 'visual') and hasattr(self.clip.visual, 'output_dim'):
            self.embed_dim = self.clip.visual.output_dim
        else:
            # å‹•æ…‹æ¨æ–·ï¼šåšä¸€æ¬¡ forward pass
            with torch.no_grad():
                dummy_input = torch.zeros(1, 3, 224, 224)
                dummy_output = self.encoder(dummy_input)
                if dummy_output.dim() > 2:
                    dummy_output = dummy_output.mean(dim=1)
                self.embed_dim = dummy_output.shape[-1]
            
        print(f"CLIP embed_dim: {self.embed_dim}")
        
        # å‡çµ encoder
        if freeze_encoder:
            self.freeze_backbone()
        
        # åˆ†é¡é ­
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.embed_dim),
            nn.Dropout(dropout),
            nn.Linear(self.embed_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )
        
        self._model_name = f"CLIP_{clip_model.replace('-', '_')}"
        
        # åˆªé™¤ä¸éœ€è¦çš„ text encoder ä»¥ç¯€çœè¨˜æ†¶é«”
        del self.clip.transformer
        del self.clip.token_embedding
        del self.clip.ln_final
        if hasattr(self.clip, 'text_projection'):
            del self.clip.text_projection
        
    def freeze_backbone(self):
        """å‡çµ CLIP encoder"""
        for param in self.encoder.parameters():
            param.requires_grad = False
        print("ğŸ”’ CLIP encoder frozen")
        
    def unfreeze_backbone(self):
        """è§£å‡ CLIP encoder"""
        for param in self.encoder.parameters():
            param.requires_grad = True
        print("ğŸ”“ CLIP encoder unfrozen")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: [B, 3, H, W] è¼¸å…¥åœ–åƒ
            
        Returns:
            [B, num_classes] logits
        """
        # CLIP visual encoder
        with torch.set_grad_enabled(not self.freeze_encoder):
            features = self.encoder(x)
        
        # ç¢ºä¿æ˜¯ 2D tensor [B, embed_dim]
        if features.dim() > 2:
            features = features.mean(dim=1)  # å¦‚æœæ˜¯ [B, N, D]ï¼Œå–å¹³å‡
        
        # åˆ†é¡
        logits = self.classifier(features)
        
        return logits
    
    def get_classifier(self) -> nn.Module:
        return self.classifier
    
    def get_backbone(self) -> nn.Module:
        return self.encoder
    
    def get_preprocessing_config(self) -> Dict[str, Any]:
        """CLIP ä½¿ç”¨è‡ªå·±çš„é è™•ç†åƒæ•¸"""
        # OpenCLIP çš„æ¨™æº–åŒ–åƒæ•¸
        return {
            'mean': [0.48145466, 0.4578275, 0.40821073],
            'std': [0.26862954, 0.26130258, 0.27577711],
            'input_size': 224,
            'interpolation': 'bicubic',
        }
    
    def get_param_groups(
        self, 
        base_lr: float = 1e-4,
        backbone_lr_scale: float = 0.0  # é è¨­å‡çµ backbone
    ) -> List[Dict[str, Any]]:
        """
        ç²å–åƒæ•¸çµ„ï¼Œæ”¯æ´åˆ†å±¤å­¸ç¿’ç‡
        """
        param_groups = []
        
        # Encoder åƒæ•¸ï¼ˆå¦‚æœæ²’å‡çµï¼‰
        if not self.freeze_encoder:
            encoder_params = [p for p in self.encoder.parameters() if p.requires_grad]
            if encoder_params:
                param_groups.append({
                    'params': encoder_params,
                    'lr': base_lr * backbone_lr_scale,
                    'name': 'clip_encoder'
                })
        
        # Classifier åƒæ•¸
        classifier_params = list(self.classifier.parameters())
        param_groups.append({
            'params': classifier_params,
            'lr': base_lr,
            'name': 'classifier'
        })
        
        return param_groups


class CLIPWithDCT(BaseModel):
    """
    CLIP Visual Encoder + DCT Frequency Features
    
    çµåˆ CLIP çš„èªæ„ç‰¹å¾µå’Œ DCT çš„é »ç‡ç‰¹å¾µ
    """
    
    def __init__(
        self,
        clip_model: str = 'ViT-B-32',
        pretrained: str = 'openai',
        num_classes: int = 2,
        dropout: float = 0.5,
        freeze_encoder: bool = True,
        dct_dim: int = 128,
        fusion_dim: int = 512,
    ):
        super().__init__(num_classes=num_classes)
        
        from .dct import DCTFeatureExtractor
        
        self.clip_model_name = clip_model
        self.freeze_encoder = freeze_encoder
        
        # è¼‰å…¥ CLIP
        print(f"Loading CLIP: {clip_model} ({pretrained})")
        self.clip, _, _ = open_clip.create_model_and_transforms(
            clip_model, pretrained=pretrained
        )
        self.encoder = self.clip.visual
        
        # ç²å–ç‰¹å¾µç¶­åº¦
        if hasattr(self.encoder, 'output_dim'):
            self.embed_dim = self.encoder.output_dim
        elif hasattr(self.encoder, 'embed_dim'):
            self.embed_dim = self.encoder.embed_dim
        elif hasattr(self.clip, 'visual') and hasattr(self.clip.visual, 'output_dim'):
            self.embed_dim = self.clip.visual.output_dim
        else:
            # å‹•æ…‹æ¨æ–·
            with torch.no_grad():
                dummy_input = torch.zeros(1, 3, 224, 224)
                dummy_output = self.encoder(dummy_input)
                if dummy_output.dim() > 2:
                    dummy_output = dummy_output.mean(dim=1)
                self.embed_dim = dummy_output.shape[-1]
            
        print(f"CLIP embed_dim: {self.embed_dim}")
        
        # å‡çµ encoder
        if freeze_encoder:
            self.freeze_backbone()
        
        # DCT ç‰¹å¾µæå–å™¨
        self.dct_extractor = DCTFeatureExtractor(output_dim=dct_dim)
        
        # èåˆå±¤
        total_dim = self.embed_dim + dct_dim
        self.fusion = nn.Sequential(
            nn.LayerNorm(total_dim),
            nn.Linear(total_dim, fusion_dim),
            nn.GELU(),
            nn.Dropout(dropout),
        )
        
        # åˆ†é¡é ­
        self.classifier = nn.Sequential(
            nn.Linear(fusion_dim, fusion_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(fusion_dim // 2, num_classes)
        )
        
        self._model_name = f"CLIP_{clip_model.replace('-', '_')}_DCT"
        
        # åˆªé™¤ text encoder
        del self.clip.transformer
        del self.clip.token_embedding
        del self.clip.ln_final
        if hasattr(self.clip, 'text_projection'):
            del self.clip.text_projection
    
    def freeze_backbone(self):
        """å‡çµ CLIP encoder"""
        for param in self.encoder.parameters():
            param.requires_grad = False
        print("ğŸ”’ CLIP encoder frozen")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # CLIP features
        with torch.set_grad_enabled(not self.freeze_encoder):
            clip_features = self.encoder(x)
        
        if clip_features.dim() > 2:
            clip_features = clip_features.mean(dim=1)
        
        # DCT features
        dct_features = self.dct_extractor(x)
        
        # èåˆ
        combined = torch.cat([clip_features, dct_features], dim=1)
        fused = self.fusion(combined)
        
        # åˆ†é¡
        logits = self.classifier(fused)
        
        return logits
    
    def get_classifier(self) -> nn.Module:
        return self.classifier
    
    def get_backbone(self) -> nn.Module:
        return self.encoder
    
    def get_preprocessing_config(self) -> Dict[str, Any]:
        return {
            'mean': [0.48145466, 0.4578275, 0.40821073],
            'std': [0.26862954, 0.26130258, 0.27577711],
            'input_size': 224,
            'interpolation': 'bicubic',
        }
    
    def get_param_groups(
        self, 
        base_lr: float = 1e-4,
        backbone_lr_scale: float = 0.0
    ) -> List[Dict[str, Any]]:
        param_groups = []
        
        # Encoder åƒæ•¸
        if not self.freeze_encoder:
            encoder_params = [p for p in self.encoder.parameters() if p.requires_grad]
            if encoder_params:
                param_groups.append({
                    'params': encoder_params,
                    'lr': base_lr * backbone_lr_scale,
                    'name': 'clip_encoder'
                })
        
        # DCT åƒæ•¸
        param_groups.append({
            'params': list(self.dct_extractor.parameters()),
            'lr': base_lr * 0.5,
            'name': 'dct'
        })
        
        # Fusion + Classifier
        param_groups.append({
            'params': list(self.fusion.parameters()) + list(self.classifier.parameters()),
            'lr': base_lr,
            'name': 'fusion_classifier'
        })
        
        return param_groups
